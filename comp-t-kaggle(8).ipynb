{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"}],"dockerImageVersionId":30716,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchaudio pandas\n\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\nimport pandas as pd\nimport torchaudio\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:19:53.072331Z","iopub.execute_input":"2024-06-07T08:19:53.072748Z","iopub.status.idle":"2024-06-07T08:20:15.833841Z","shell.execute_reply.started":"2024-06-07T08:19:53.072712Z","shell.execute_reply":"2024-06-07T08:20:15.832327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Charger les métadonnées\ntrain_metadata = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\ntaxonomy = pd.read_csv('/kaggle/input/birdclef-2024/eBird_Taxonomy_v2021.csv')\nsample_submission = pd.read_csv('/kaggle/input/birdclef-2024/sample_submission.csv')\n\n# Afficher les premières lignes des métadonnées\nprint(train_metadata.head())\nprint(taxonomy.head())\nprint(sample_submission.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:20:15.836763Z","iopub.execute_input":"2024-06-07T08:20:15.837341Z","iopub.status.idle":"2024-06-07T08:20:16.198352Z","shell.execute_reply.started":"2024-06-07T08:20:15.837303Z","shell.execute_reply":"2024-06-07T08:20:16.196748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchaudio\nimport pandas as pd\nimport os\n\n# Charger les métadonnées pour déterminer le nombre de classes\ntrain_metadata = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')\nnum_classes = train_metadata['primary_label'].nunique()\n\nclass CNNNetwork(nn.Module):\n    def __init__(self, num_classes):\n        super(CNNNetwork, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(0.5)\n        self.linear = None  # Placeholder, we will set this later\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        print(\"Shape after conv1:\", x.shape)\n        x = self.conv2(x)\n        print(\"Shape after conv2:\", x.shape)\n        x = self.conv3(x)\n        print(\"Shape after conv3:\", x.shape)\n        x = self.conv4(x)\n        print(\"Shape after conv4:\", x.shape)\n        x = self.flatten(x)\n        print(\"Shape after flatten:\", x.shape)\n        x = self.dropout(x)\n\n        # Initialize linear layer based on flattened size\n        if self.linear is None:\n            self.linear = nn.Linear(x.shape[1], num_classes).to(x.device)\n        \n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:47:15.453436Z","iopub.execute_input":"2024-06-07T08:47:15.454154Z","iopub.status.idle":"2024-06-07T08:47:15.599310Z","shell.execute_reply.started":"2024-06-07T08:47:15.454116Z","shell.execute_reply":"2024-06-07T08:47:15.598194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\n# Définissez votre configuration\nANNOTATIONS_FILE = '/kaggle/input/birdclef-2024/train_metadata.csv'\nAUDIO_DIR = '/kaggle/input/birdclef-2024/train_audio'\nFOLDER_FILTERS = ['barfly1', 'asbfly', 'bkrfla1', 'brakit1', 'categr']\nSAMPLE_RATE = 16000\nNUM_SAMPLES = 16000\nBATCH_SIZE = 32\nEPOCHS = 20\nLEARNING_RATE = 0.001\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Définir le dataset\nclass BirdCLEFDataset(Dataset):\n    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples, device, folder_filters=None):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n        \n        if folder_filters is not None:\n            print(f\"Applying folder filters: {', '.join(folder_filters)}\")\n            initial_count = len(self.annotations)\n            self.annotations = self.annotations[self.annotations['filename'].str.contains('|'.join(folder_filters))]\n            filtered_count = len(self.annotations)\n            if filtered_count == 0:\n                raise ValueError(f\"No files found in folders: {', '.join(folder_filters)}\")\n            print(f\"Filtered dataset from {initial_count} to {filtered_count} samples.\")\n\n        self.annotations.reset_index(drop=True, inplace=True)\n        self.label_to_index = {label: idx for idx, label in enumerate(self.annotations['primary_label'].unique())}\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        filename = self.annotations.iloc[index]['filename']\n        path = os.path.join(self.audio_dir, filename)\n        return path\n\n    def _get_audio_sample_label(self, index):\n        label = self.annotations.iloc[index]['primary_label']\n        label_index = self.label_to_index[label]\n        return label_index\n\nmel_spectrogram = torchaudio.transforms.MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_mels=64,\n    n_fft=1024,\n    hop_length=512\n)\n\n# Charger toutes les annotations et filtrer si nécessaire\ndataset = BirdCLEFDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device, folder_filters=FOLDER_FILTERS)\n\n# Diviser le dataset en ensemble d'entraînement et de validation\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nclass CNNNetwork(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(64 * 16 * 8, 128)  # Corrected dimensions here\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n# Instancier le modèle\nnum_classes = len(dataset.label_to_index)\nmodel = CNNNetwork(num_classes=num_classes).to(device)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Fonction de formation\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        \n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 10 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n# Fonction de test\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    return correct\n\n# Entraîner le modèle\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n    train(train_loader, model, loss_fn, optimizer)\n    accuracy = test(val_loader, model, loss_fn)\n    print(f\"Validation Accuracy: {accuracy:.2f}\")\n\n# Sauvegarder le modèle entraîné\ntorch.save(model.state_dict(), 'birdclef_cnn_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T09:05:46.436293Z","iopub.execute_input":"2024-06-07T09:05:46.436729Z","iopub.status.idle":"2024-06-07T09:17:14.968524Z","shell.execute_reply.started":"2024-06-07T09:05:46.436695Z","shell.execute_reply":"2024-06-07T09:17:14.967245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport pandas as pd\nimport torch.nn as nn\n\nclass CNNNetwork(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(64 * 16 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndef process_audio_file(file_path, transformation, target_sample_rate, num_samples, device):\n    signal, sr = torchaudio.load(file_path)\n    signal = signal.to(device)\n    if sr != target_sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, target_sample_rate).to(device)\n        signal = resampler(signal)\n    if signal.shape[1] > num_samples:\n        signal = signal[:, :num_samples]\n    elif signal.shape[1] < num_samples:\n        num_missing_samples = num_samples - signal.shape[1]\n        signal = torch.nn.functional.pad(signal, (0, num_missing_samples))\n    signal = transformation(signal)\n    return signal\n\ndef predict(model, input, class_mapping):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(input)\n        predicted_index = predictions[0].argmax(0)\n        predicted = class_mapping[predicted_index.item()]\n    return predicted\n\ndef main():\n    # Définir les dossiers spécifiques\n    FOLDER_FILTERS = ['barfly1', 'asbfly', 'bkrfla1', 'brakit1', 'categr']\n\n    # Charger les métadonnées pour déterminer le nombre de classes\n    ANNOTATIONS_FILE = '/kaggle/input/birdclef-2024/train_metadata.csv'\n    train_metadata = pd.read_csv(ANNOTATIONS_FILE)\n    filtered_metadata = train_metadata[train_metadata['filename'].str.contains('|'.join(FOLDER_FILTERS))]\n    num_classes = filtered_metadata['primary_label'].nunique()\n    class_mapping = {label: i for i, label in enumerate(filtered_metadata['primary_label'].unique())}\n    inv_class_mapping = {v: k for k, v in class_mapping.items()}\n\n    # Charger le modèle\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = CNNNetwork(num_classes=num_classes).to(device)\n    \n    # Charger l'état du modèle\n    state_dict = torch.load('birdclef_cnn_model.pth', map_location=device)\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    # Transformation\n    SAMPLE_RATE = 16000\n    NUM_SAMPLES = 16000\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_mels=64,\n        n_fft=1024,\n        hop_length=512\n    ).to(device)\n\n    # Parcourir les dossiers spécifiés et prédire les classes\n    audio_dir = '/kaggle/input/birdclef-2024/train_audio'\n    results = []\n\n    for folder in FOLDER_FILTERS:\n        folder_path = os.path.join(audio_dir, folder)\n        if os.path.exists(folder_path):\n            print(f\"Processing folder: {folder}\")  # Log the folder being processed\n            for root, _, files in os.walk(folder_path):\n                for file in files:\n                    if file.endswith('.ogg'):\n                        file_path = os.path.join(root, file)\n                        print(f\"Processing file: {file_path}\")  # Log the file being processed\n                        audio_tensor = process_audio_file(file_path, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, device)\n                        audio_tensor = audio_tensor.unsqueeze(0)  # Ajouter une dimension pour le batch\n                        predicted_class = predict(model, audio_tensor, inv_class_mapping)\n                        filename = os.path.basename(file_path)  # Extraire le nom du fichier\n                        results.append({'filename': filename, 'bird_species': predicted_class})\n                        print(f\"Predicted class for {file_path}: {predicted_class}\")  # Log the prediction\n        else:\n            print(f\"Folder {folder} does not exist in the audio directory.\")\n\n    # Créer le fichier de soumission\n    submission = pd.DataFrame(results)\n    submission.to_csv('submission.csv', index=False)\n\n    print(\"Fichier de soumission créé : submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-06-07T09:34:02.784206Z","iopub.execute_input":"2024-06-07T09:34:02.784662Z","iopub.status.idle":"2024-06-07T09:34:37.351552Z","shell.execute_reply.started":"2024-06-07T09:34:02.784629Z","shell.execute_reply":"2024-06-07T09:34:37.350046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}